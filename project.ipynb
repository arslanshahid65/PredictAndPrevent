{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16495eb",
   "metadata": {},
   "source": [
    "**Case Study:** *A Comprehensive industrial component degradation modelling study for predictive maintainance.*\n",
    "    \n",
    "[Dataset](https://www.kaggle.com/datasets/inIT-OWL/one-year-industrial-component-degradation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fcbb96",
   "metadata": {},
   "source": [
    "**Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data from source\n",
    "\n",
    "! pip install -q gdown\n",
    "\n",
    "import gdown\n",
    "\n",
    "file_id = \"1B8lP4fY09ohS6-Z9uIrjIbP7cqp8NPcO\"\n",
    "\n",
    "# Construct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Download ZIP\n",
    "output = \"Dataset_One_Year_Component_Degradation.zip\"\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "# Unzip to current directory\n",
    "!unzip -o Dataset_One_Year_Component_Degradation.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b3dde",
   "metadata": {},
   "source": [
    "**Optional: Download Data Directly from Kaggle (Upstream Source)**\n",
    "\n",
    "*Works only in colab environment*\n",
    "\n",
    "The section downloads the dataset directly from the upstream source (Kaggle) to ensure you always get the latest version. Requires your own kaggle.json credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e428cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54541c1c",
   "metadata": {},
   "source": [
    "Generate an API token from your kaggle account:\n",
    "Account > API > Create New Token.\n",
    "\n",
    "This generates a kaggle.json file for authentication required for loading data directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0da053",
   "metadata": {},
   "source": [
    "Upload your kaggle.json to this colab runtime instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474842c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ~/.kaggle\n",
    "! mv kaggle.json ~/.kaggle/\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb22eba",
   "metadata": {},
   "source": [
    "Ingesting data from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fa6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets download inIT-OWL/one-year-industrial-component-degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e064a0a",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdeda39",
   "metadata": {},
   "source": [
    "1) Redundant Features and Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f030199",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_corr_matric = raw_df.corr()\n",
    "\n",
    "sns.heatmap(raw_df_corr_matric,\n",
    "xticklabels=raw_df_corr_matric.columns,\n",
    "yticklabels=raw_df_corr_matric.columns)\n",
    "\n",
    "AFTER_DROPPING_CORRELATED_COLUNMS = ['pCut::Motor_Torque',\n",
    "'pCut::CTRL_Position_controller::Actual_position',\n",
    "'pSvolFilm::CTRL_Position_controller::Actual_position',\n",
    "'pSvolFilm::CTRL_Position_controller::Lag_error', 'pSpintor::VAX_speed']\n",
    "\n",
    "dropped_colunm_df = raw_df[AFTER_DROPPING_CORRELATED_COLUNMS]\n",
    "drop_df_corr_matric = dropped_colunm_df.corr()\n",
    "sns.heatmap(drop_df_corr_matric,\n",
    "xticklabels=drop_df_corr_matric.columns,\n",
    "yticklabels=drop_df_corr_matric.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c043c",
   "metadata": {},
   "source": [
    "2) Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4577302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "scaled_df[COLUNMS_TO_BE_SCALED] =\n",
    "scaler.fit_transform(raw_df[COLUNMS_TO_BE_SCALED])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2623e",
   "metadata": {},
   "source": [
    "3) Feature Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37aca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scalar = preprocessing.StandardScaler()\n",
    "standardized_df = raw_df.copy()\n",
    "standard_scalar.fit(raw_df[COLUNMS_TO_BE_SCALED])\n",
    "standardized_df[COLUNMS_TO_BE_SCALED] =\n",
    "standard_scalar.transform(raw_df[COLUNMS_TO_BE_SCALED])\n",
    "standardized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f85e3",
   "metadata": {},
   "source": [
    "**Modelling Using Auto Encoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4176c",
   "metadata": {},
   "source": [
    "1) Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa127042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder32x(tf.keras.Model):\n",
    "def __init__(self,latent_dim,input_dim):\n",
    "    super(Autoencoder32x, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.input_dim = input_dim\n",
    "    self.dropout_factor = 0.3\n",
    "    self.encoder = Sequential([\n",
    "                    Dense(32, activation='elu',input_shape=(self.input_dim,)),\n",
    "                    Dense(16, activation='elu'),\n",
    "                    Dense(8, activation='elu'),\n",
    "                    Dense(self.latent_dim, activation='elu')\n",
    "    ])\n",
    "    self.decoder = Sequential([\n",
    "                    Dense(8, activation='elu', input_shape=(self.latent_dim,)),\n",
    "                    Dense(16, activation='elu'),\n",
    "                    Dense(32, activation='elu'),\n",
    "                    Dense(self.input_dim, activation=None)\n",
    "    ])\n",
    "def call(self, inputs):\n",
    "    encoder_out = self.encoder(inputs)\n",
    "    return self.decoder(encoder_out)\n",
    "    # This is the dimension of the latent space (encoding space)\n",
    "latent_dim = 2\n",
    "autoencoder_10_32x =\n",
    "Autoencoder32x(latent_dim=latent_dim,input_dim=len(COLUNMS_FOR_AUTOENCODER))\n",
    "autoencoder_10_32x.compile(loss='mse', optimizer='adam',metrics=['accuracy'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17126b03",
   "metadata": {},
   "source": [
    "2) Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca39fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "ae_train_x, ae_test_x, ae_train_y, ae_test_y =\n",
    "train_test_split(standardized_df[COLUNMS_FOR_AUTOENCODER],\n",
    "standardized_df[COLUNMS_FOR_AUTOENCODER], test_size=0.33)\n",
    "\n",
    "# training\n",
    "ae_history = autoencoder_10_32x.fit(ae_train_x, ae_train_y,\n",
    "validation_data=(ae_test_x, ae_test_y), epochs=100)\n",
    "plt.subplots(figsize=(16,10))\n",
    "plt.plot(ae_history.history.get('accuracy'),label='Train Accuracy')\n",
    "plt.plot(ae_history.history.get('val_accuracy'),label=\"Test Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dffc7b",
   "metadata": {},
   "source": [
    "3) Prediction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec803d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prediction_error(x,y):\n",
    "    diff = x-y\n",
    "    error = (diff**2).mean(axis=1)**0.5  \n",
    "    return error\n",
    "\n",
    "predicted_32x_ae_output =\n",
    "autoencoder_10_32x.predict(standardized_df[COLUNMS_FOR_AUTOENCODER])\n",
    "standardized_df['ae_error_32x_10'] =\n",
    "find_prediction_error(standardized_df[COLUNMS_FOR_AUTOENCODER],predicted_3\n",
    "2x_ae_output)\n",
    "plt.subplots(figsize=(16,10))\n",
    "plt.plot(standardized_df['ae_error_32x_10'],label='Prediction Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98c581",
   "metadata": {},
   "source": [
    "4) Low Pass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b03fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "order = 6\n",
    "fs = 1100\n",
    "cutoff = 0.8\n",
    "b, a = butter_lowpass(cutoff, fs, order)\n",
    "w, h = freqz(b, a, worN=8000)\n",
    "plt.plot(0.5*fs*w/np.pi, np.abs(h), 'b')\n",
    "plt.plot(cutoff, 0.5*np.sqrt(2), 'ko')\n",
    "plt.axvline(cutoff, color='k')\n",
    "plt.xlim(0, 0.5*fs)\n",
    "plt.title(\"Lowpass Filter Frequency Response\")\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.grid()\n",
    "T = 4152\n",
    "n = 1062912\n",
    "t = np.linspace(0, T, n, endpoint=False)\n",
    "low_pass_filtered_error =\n",
    "butter_lowpass_filter(standardized_df['ae_error_32x_10'], cutoff, fs,\n",
    "order)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.subplots(figsize=(16,10))\n",
    "plt.plot(t, standardized_df['ae_error_32x_10'], 'b-', label='data')\n",
    "plt.plot(t, low_pass_filtered_error, 'g-', linewidth=2, label='filtered\n",
    "data')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.subplots_adjust(hspace=0.35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d79b9",
   "metadata": {},
   "source": [
    "5) Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_error_std = low_pass_filtered_error.std()\n",
    "filtered_error_mean = low_pass_filtered_error.mean()\n",
    "ae_threshold_line = np.array([filtered_error_std+filtered_error_mean for i\n",
    "in range(len(low_pass_filtered_error))])\n",
    "plt.subplots(figsize=(16,10))\n",
    "plt.plot(low_pass_filtered_error,label='Prediction Error')\n",
    "plt.plot(ae_threshold_line,label='First STD Threshold')\n",
    "plt\n",
    "plt.legend()\n",
    "ae_threshold_line_std_2 =\n",
    "np.array([2*filtered_error_std+filtered_error_mean for i in\n",
    "range(len(low_pass_filtered_error))])\n",
    "plt.subplots(figsize=(16,10))\n",
    "plt.plot(low_pass_filtered_error,label='Prediction Error')\n",
    "plt.plot(ae_threshold_line_std_2,label='Second STD Threshold')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50745b7",
   "metadata": {},
   "source": [
    "**Modelling using Self Organizing Maps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21b100",
   "metadata": {},
   "source": [
    "1) Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New component data as training dataset\n",
    "new_blade_data_standard_scalar = preprocessing.StandardScaler()\n",
    "new_blade_standardized = new_blade_raw.copy()\n",
    "new_blade_data_standard_scalar.fit(new_blade_raw[COLUNMS_FOR_SOM])\n",
    "new_blade_standardized[COLUNMS_FOR_SOM] =\n",
    "new_blade_data_standard_scalar.transform(new_blade_raw[COLUNMS_FOR_SOM])\n",
    "som_train_data =np.array(new_blade_standardized[COLUNMS_FOR_SOM])\n",
    "\n",
    "#lifecycle data as testing dataset\n",
    "lc_standard_df_scalar = preprocessing.StandardScaler()\n",
    "lc_standard_df = raw_df.copy()\n",
    "lc_standard_df_scalar.fit(raw_df[COLUNMS_FOR_SOM])\n",
    "lc_standard_df[COLUNMS_FOR_SOM] =\n",
    "lc_standard_df_scalar.transform(raw_df[COLUNMS_FOR_SOM])\n",
    "som_test_data =np.array(lc_standard_df[COLUNMS_FOR_SOM])\n",
    "rows_data = som_train_data.shape[0]\n",
    "x = int(np.sqrt(5*np.sqrt(rows_data)))\n",
    "y = x\n",
    "input_len = som_train_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b95982",
   "metadata": {},
   "source": [
    "2) Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27528236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_som(x, y, input_len, sigma, learning_rate,iterations):\n",
    "    som = MiniSom(x=x,\n",
    "    y=y,\n",
    "    input_len = input_len,\n",
    "    sigma=sigma,\n",
    "    learning_rate=learning_rate)\n",
    "    som.random_weights_init(som_train_data)\n",
    "    #training\n",
    "    start_time = time.time()\n",
    "    som.train_batch(som_train_data, iterations) # trains the SOM with 100\n",
    "    iterations\n",
    "    elapsed_time = time.time()-start_time\n",
    "    print(elapsed_time, \" seconds\")\n",
    "    return som"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023931cf",
   "metadata": {},
   "source": [
    "3) Sigma and Learning Rate Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c00590",
   "metadata": {},
   "outputs": [],
   "source": [
    "space={\n",
    "'sig' : hp.uniform(\"sig\", 0.001, 3),\n",
    "'learning_rate' : hp.uniform(\"learning_rate\", 0.001, 10)\n",
    "}\n",
    "\n",
    "def som_fn(space):\n",
    "    sig = space['sig']\n",
    "    lr = space['learning_rate']\n",
    "    val = MiniSom(x=x,y=y,input_len =\n",
    "    input_len,sigma=sig,learning_rate=lr).quantization_error(som_train_data)\n",
    "    print(val)\n",
    "    return {'loss':val, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn = som_fn,\n",
    "    space = space,\n",
    "    algo = tpe.suggest,\n",
    "    max_evals = 1000,\n",
    "        trials = trials)\n",
    "print('best: {}'.format(best))\n",
    "\n",
    "for i, trial in enumerate(trials.trials[0:2]):\n",
    "    #update sigma and learning\n",
    "    sigma = best['sig']\n",
    "    learning_rate = best['learning_rate']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804907f6",
   "metadata": {},
   "source": [
    "4) Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "som = train_som(x,y,input_len=input_len,sigma=sigma,learning_rate=learning_rate,\n",
    "        iterations=iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5024d2",
   "metadata": {},
   "source": [
    "5) SOM Grid Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45161540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import plot, axis, show, pcolor, colorbar, bone\n",
    "plt.figure(figsize=(16,12))\n",
    "bone()\n",
    "pcolor(som.distance_map().T) #distance map as background\n",
    "colorbar()\n",
    "#use different color and markers for each label\n",
    "markers = ['o', 's', 'D']\n",
    "colors = ['r', 'g', 'b']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bfc39f",
   "metadata": {},
   "source": [
    "6) Degradation Visualization using life cycle data and trained SOM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9542be",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_errors = np.array([])\n",
    "quantization_errors = quantization_errors.astype(int)\n",
    "for i in range(6000,np.shape(som_test_data)[0],6000):\n",
    "    quantization_errors = np.append(quantization_errors,\n",
    "    np.linalg.norm(som.quantization(som_test_data[i-6000:i]) -\n",
    "                som_test_data[i-6000:i], axis=1))\n",
    "\n",
    "#moving average to remove noise from data\n",
    "kernel_size = 2000\n",
    "kernel = np.ones(kernel_size) / kernel_size\n",
    "#quantization_errors = np.convolve(quantization_errors, kernel,\n",
    "mode='same')\n",
    "plt.subplots(figsize=(16,10))\n",
    "print(type(quantization_errors))\n",
    "plt.plot(quantization_errors)\n",
    "print(np.shape(som_test_data)[0])\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Quantization Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934163e2",
   "metadata": {},
   "source": [
    "7) Threshold Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = quantization_errors\n",
    "\n",
    "def normal_dist(x , mean , sd):\n",
    "    prob_density = (np.pi*sd) * np.exp(-0.5*((x-mean)/sd)**2)\n",
    "    return prob_density\n",
    "\n",
    "#Calculate mean and Standard deviation.\n",
    "mean = np.mean(z)\n",
    "sd = np.std(z)\n",
    "#Apply function to the data to find normal distribution.\n",
    "pdf = normal_dist(z,mean,sd)\n",
    "# Threshold as Error value for Τ = 0.99 from the distribution\n",
    "threshold = np.quantile(z, 0.99)\n",
    "print(\"Error value for Τ = 0.99 from the distribution : \", threshold)\n",
    "#Plotting the Results\n",
    "plt.subplots(figsize=(16,10))\n",
    "plt.plot(z,pdf , color = 'blue')\n",
    "plt.plot([threshold,threshold], [0,3], color = 'red')\n",
    "plt.xlabel('Error Values')\n",
    "plt.ylabel('Probability Density')\n",
    "quantization_errors = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a9de2",
   "metadata": {},
   "source": [
    "8) Visualization with Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d981735",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6000,np.shape(som_test_data)[0],6000):\n",
    "        quantization_errors = np.append(quantization_errors,\n",
    "            np.linalg.norm(som.quantization(som_test_data[i-6000:i]) -\n",
    "            som_test_data[i-6000:i], axis=1))\n",
    "\n",
    "plt.subplots(figsize=(16,10))\n",
    "plt.plot(quantization_errors)\n",
    "plt.plot([6000,np.shape(som_test_data)[0]], [threshold,threshold])\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Quantization Error')\n",
    "Visualization With Threshold line(Zoomed in):\n",
    "plt.subplots(figsize=(32,20))\n",
    "plt.plot(quantization_errors[122500-500:122500])\n",
    "plt.plot([0,500], [threshold, threshold])\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Quantization Error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
